{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functionality\n",
    "\n",
    "1. Resize test images into size (300, 300)\n",
    "\n",
    "2. Load trained Resnet101 based model from version 2 (resnet101-weights/resnet101_weights_tf_dim_ordering_tf_kernels_notop.h5) of the same kernel \n",
    "\n",
    "3. nTTA = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aptos2019-blindness-detection', 'resnet101-weights', 'aptoskeras-resnet101']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# Load some \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io\n",
    "from skimage.transform import resize\n",
    "from imgaug import augmenters as iaa\n",
    "from tqdm import tqdm\n",
    "import PIL\n",
    "from PIL import Image, ImageOps\n",
    "import cv2\n",
    "from sklearn.utils import class_weight, shuffle\n",
    "from keras.losses import binary_crossentropy, categorical_crossentropy\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import f1_score, fbeta_score, cohen_kappa_score, accuracy_score\n",
    "from keras.utils import Sequence\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import (Activation, Dropout, Flatten, Dense, GlobalMaxPooling2D,\n",
    "                          BatchNormalization, Input, Conv2D, GlobalAveragePooling2D)\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras_applications.resnet import ResNet101\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import metrics\n",
    "from keras.optimizers import Adam \n",
    "from keras import backend as K\n",
    "import keras\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('../input/aptos2019-blindness-detection/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Resnet101_bestqwk.h5 from Version2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.legacy import interfaces\n",
    "from keras.optimizers import Optimizer\n",
    "from keras import backend as K\n",
    "class AdamAccumulate(Optimizer):\n",
    "    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999,\n",
    "                 epsilon=None, decay=0., amsgrad=False, accum_iters=2, **kwargs):\n",
    "        if accum_iters < 1:\n",
    "            raise ValueError('accum_iters must be >= 1')\n",
    "        super(AdamAccumulate, self).__init__(**kwargs)\n",
    "        with K.name_scope(self.__class__.__name__):\n",
    "            self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
    "            self.lr = K.variable(lr, name='lr')\n",
    "            self.beta_1 = K.variable(beta_1, name='beta_1')\n",
    "            self.beta_2 = K.variable(beta_2, name='beta_2')\n",
    "            self.decay = K.variable(decay, name='decay')\n",
    "        if epsilon is None:\n",
    "            epsilon = K.epsilon()\n",
    "        self.epsilon = epsilon\n",
    "        self.initial_decay = decay\n",
    "        self.amsgrad = amsgrad\n",
    "        self.accum_iters = K.variable(accum_iters, K.dtype(self.iterations))\n",
    "        self.accum_iters_float = K.cast(self.accum_iters, K.floatx())\n",
    "\n",
    "    @interfaces.legacy_get_updates_support\n",
    "    def get_updates(self, loss, params):\n",
    "        grads = self.get_gradients(loss, params)\n",
    "        self.updates = [K.update_add(self.iterations, 1)]\n",
    "\n",
    "        lr = self.lr\n",
    "\n",
    "        completed_updates = K.cast(K.tf.floor(self.iterations / self.accum_iters), K.floatx())\n",
    "\n",
    "        if self.initial_decay > 0:\n",
    "            lr = lr * (1. / (1. + self.decay * completed_updates))\n",
    "\n",
    "        t = completed_updates + 1\n",
    "\n",
    "        lr_t = lr * (K.sqrt(1. - K.pow(self.beta_2, t)) / (1. - K.pow(self.beta_1, t)))\n",
    "\n",
    "        # self.iterations incremented after processing a batch\n",
    "        # batch:              1 2 3 4 5 6 7 8 9\n",
    "        # self.iterations:    0 1 2 3 4 5 6 7 8\n",
    "        # update_switch = 1:        x       x    (if accum_iters=4)\n",
    "        update_switch = K.equal((self.iterations + 1) % self.accum_iters, 0)\n",
    "        update_switch = K.cast(update_switch, K.floatx())\n",
    "\n",
    "        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        gs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "\n",
    "        if self.amsgrad:\n",
    "            vhats = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        else:\n",
    "            vhats = [K.zeros(1) for _ in params]\n",
    "\n",
    "        self.weights = [self.iterations] + ms + vs + vhats\n",
    "\n",
    "        for p, g, m, v, vhat, tg in zip(params, grads, ms, vs, vhats, gs):\n",
    "\n",
    "            sum_grad = tg + g\n",
    "            avg_grad = sum_grad / self.accum_iters_float\n",
    "\n",
    "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * avg_grad\n",
    "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(avg_grad)\n",
    "\n",
    "            if self.amsgrad:\n",
    "                vhat_t = K.maximum(vhat, v_t)\n",
    "                p_t = p - lr_t * m_t / (K.sqrt(vhat_t) + self.epsilon)\n",
    "                self.updates.append(K.update(vhat, (1 - update_switch) * vhat + update_switch * vhat_t))\n",
    "            else:\n",
    "                p_t = p - lr_t * m_t / (K.sqrt(v_t) + self.epsilon)\n",
    "\n",
    "            self.updates.append(K.update(m, (1 - update_switch) * m + update_switch * m_t))\n",
    "            self.updates.append(K.update(v, (1 - update_switch) * v + update_switch * v_t))\n",
    "            self.updates.append(K.update(tg, (1 - update_switch) * sum_grad))\n",
    "            new_p = p_t\n",
    "\n",
    "            # Apply constraints.\n",
    "            if getattr(p, 'constraint', None) is not None:\n",
    "                new_p = p.constraint(new_p)\n",
    "\n",
    "            self.updates.append(K.update(p, (1 - update_switch) * p + update_switch * new_p))\n",
    "        return self.updates\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'lr': float(K.get_value(self.lr)),\n",
    "                  'beta_1': float(K.get_value(self.beta_1)),\n",
    "                  'beta_2': float(K.get_value(self.beta_2)),\n",
    "                  'decay': float(K.get_value(self.decay)),\n",
    "                  'epsilon': self.epsilon,\n",
    "                  'amsgrad': self.amsgrad}\n",
    "        base_config = super(AdamAccumulate, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, n_out):\n",
    "    input_tensor = Input(shape=input_shape)\n",
    "    base_model = ResNet101(include_top=False,\n",
    "                   weights=None,\n",
    "                   input_tensor=input_tensor,\n",
    "                   backend=keras.backend, layers=keras.layers, models=keras.models, utils=keras.utils)\n",
    "    base_model.load_weights('../input/resnet101-weights/resnet101_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    final_output = Dense(n_out, activation= \"softmax\", name='final_output')(x)\n",
    "    model = Model(input_tensor, final_output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagesize = 300\n",
    "model = create_model(input_shape=(imagesize,imagesize,3), n_out = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('../input/aptoskeras-resnet101/Resnet101_bestqwk.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict with TTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "def tta_prediction(model, image, n_examples):\n",
    "    \"\"\"\n",
    "        make a prediction for one image using test-time augmentation\n",
    "        \n",
    "        @param model: model for prediction\n",
    "        @param image: input image to be predicted (nrows* ncols * nchns) \n",
    "        @param n_exampls: \n",
    "        \n",
    "        @return yhat: predicted label for image (scalar int)\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    \n",
    "    # configure image data augmentation\n",
    "    datagen = ImageDataGenerator(horizontal_flip=True,rotation_range = 15,\n",
    "                                 data_format = 'channels_last')\n",
    "    \n",
    "    # convert image into dataset\n",
    "    samples = np.expand_dims(image, 0)\n",
    "    \n",
    "    # prepare iterator\n",
    "    it = datagen.flow(samples, batch_size=n_examples)\n",
    "    \n",
    "    # make predictions for each augmented image\n",
    "    yhats = model.predict_generator(it, steps=n_examples, verbose=0)\n",
    "    \n",
    "    # sum across predictions\n",
    "    summed = np.sum(yhats, axis=0)\n",
    "    \n",
    "    # argmax across classes\n",
    "    yhat = np.argmax(summed)\n",
    "    \n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1928it [09:45,  3.15it/s]\n"
     ]
    }
   ],
   "source": [
    "ntta = 10\n",
    "predicted = []\n",
    "for i, name in tqdm(enumerate(df_test['id_code'])):\n",
    "    path = os.path.join('../input/aptos2019-blindness-detection/test_images/', name+'.png')\n",
    "    image = cv2.imread(path)\n",
    "    \n",
    "    image = cv2.resize(image, (imagesize, imagesize))\n",
    "    \n",
    "    image = image/255\n",
    "    \n",
    "    label_predict = tta_prediction(model = model, image = image, n_examples = ntta)\n",
    "    predicted.append(str(label_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['diagnosis'] = predicted\n",
    "df_test.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
